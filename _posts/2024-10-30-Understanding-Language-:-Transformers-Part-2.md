
In this post, we will continue from where we left off in the last post. We will march on towards our ultimate goal of implementing a Transformer from scratch. The full implementation of the Transformer can be found [here:](https://gist.github.com/tushar-c/d0c6b51822f1daf067b51cb68788acd7)

We will use the [PyTorch Framework](https://pytorch.org/) to create our implementation of the Transformer. Also, as mentioned previously, our implementation is based on the **Attention is All you Need** Paper by **Vaswani et. al.** found [here](https://arxiv.org/abs/1706.03762).

## Introduction

In this post, we will focus on implementing all the building blocks of the Transformer Architecture. Since the Encoder and the Decoder (explained later), are parts of the Transformer, the parts that we will focus on in this post also serve as the building blocks of the Encoder and the Decoder.

Let's begin!



